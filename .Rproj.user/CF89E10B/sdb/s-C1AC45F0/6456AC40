{
    "contents" : "---\ntitle: \"Determining Level of Performance in Weight Lifting\"\nauthor: \"Rebecca Acevedo\"\ndate: \"March 22, 2016\"\noutput: html_document\n---\n\n## Overview\nThe purpose of this exercise is to examine a data set and create a prediction model to identify when a person is performing the task of a Unilateral Dumbbell Bicep Curl.  The data utilized in this exploration was obtained from the Weight Lifting Exercises Dataset (See Reference 1), which provides data that measures through the use of accelerometers of six men performing the Unilateral Dumbbell Bicep Curl to exact specifications, both correctly and incorrectly.\n\n##Initial Set-up\n### R Set-up\n```{r setup, warning=FALSE}\nlibrary(ggplot2)\nlibrary(caret)\nlibrary(pROC)\n```\n### Data Set-up\nThe data was downloaded on March 21, 2015 from the predictive machine learning library on cloudfront.  There are two sets of data, the training set and the testing set. \n#### Training Data\nThe training set of data is to be used for creating the model.\n```{r training, cache = TRUE}\ndownload.file(\"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv\",  \"pml-training.csv\")\ndata <- read.csv(\"pml-training.csv\", na.strings=c(\"#DIV/0!\"), row.names = 1)\ndata$classe <- factor(data$classe)\n```\n#### Testing Data\nThe test set is to only be used once the model is final.\n```{r test data, cache=TRUE}\ndownload.file(\"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv\", \"pml-testing.csv\")\ntesting <- read.csv(\"pml-testing.csv\", na.strings=c(\"#DIV/0!\"), row.names = 1)\n```\n#### Preparing Test Data\nA first glance is taken of the data to determine the volume of data.\n```{r dims}\ndim(data)\n```\nFrom the dimensions, we can see there are over 154 columns of data. In order to get a good prediction, the data needed to be narrowed down. An overview of the data demonstrated that the first 6 columns were inconsequential to a prediction algorythim, as they were not measures, rather they were metadata regarding the subjects, and the time at which the tests were performed. Thus these columns were removed.\n```{r }\ndata <- data[7:159]\n```\nThere still however, remained too many columns for a good prediction. The overview also revealed that there were a number of columns where the data collected was minimal or non-existant. The next step to reducing the columns was therefore to remove any columns where the percentage of captured data was less than 90%.\n```{r ninty, cache = TRUE}\nmin <- dim(data)[1]*.9\nkeep <- !apply(data, 2, function(x) sum(is.na(x)) > min || sum(x==\"\")>min || sum(x==\"NA\")>min)\ndata <- data[,keep]\n```\n\nThis resulted in a far more manageble data set of 53 columns. Having removed almost two-thirds of the columns, the next step was to test the available data.\n\n## Method\n\nAs it is unclear which predictive method will produce the best model cross-validation was used to evaluate results before determining a final model. Therefore, the first step is to divide the training data set further, into an initial training set and a cross-validation set. Sixty percent of the data as used for a training set, and the remaining 40 percent was used for cross validation.\n\n``` {r split, cache=TRUE}\ninTrain <- createDataPartition(data$classe, p=.6)[[1]]\ntraininga <- data[inTrain,]\ncross <- data[-inTrain,]\n```\n\n### Generalized Boosted Regression Model\nThe first model created was the generalized boosted regression model.\n```{r gbm, cache=TRUE, results='hide'}\ngbmmod <- train(classe ~ ., data=traininga, method=\"gbm\")\ngbmpred <- predict(gbmmod, cross)\n```\n### Linear Discriminate Analysis\nThe second modes was the linear discriminate analysis.\n```{r lda, cache=TRUE, results='hide'}\nldamod <- train(classe ~ ., data=traininga, method=\"lda\")\nldapred <- predict(ldamod, cross)\n```\n### Random Forest\nThe final model created was based on the random forest method.\n```{r rf, cache=TRUE, results='hide', warning=FALSE}\nrfmod <- train(classe ~ ., data=traininga, method=\"rf\")\nrfpred <- predict(rfmod, cross)\n```\n## Model Evaluation\n\nIn order to evaluate the best model for the final data set.  Confusion matricies were reviewed for each of fthe models. \n```{r confusion}\nconfusionMatrix(gbmpred, cross$classe)\nconfusionMatrix(ldapred, cross$classe)\nconfusionMatrix(rfpred, cross$classe)\n```\nIt is clear from the confusion matrices that the random forest, performing at 98.9% accuracy, with an out of sample error rate of 1.07%, is the best model to apply to this data. With a quick review of the most impactful data points, it is interesting to see that roll_belt plays such an integral part in the process. \n\n```{r plots, cache=TRUE, echo=FALSE}\nrfrelvars <- varImp(rfmod)\nplot(rfrelvars, main = \"Top 25 Most Impactful Variables to Random Forest Model\", top = 25)\nplot(rfmod$finalModel, main = \"Test\")\n```\n\n## Final Test\n\nFor the final step of the project, after the decision was made to use the random forest model for predicting, the model was run against the testing data\n\n```{r final test}\ntest <- predict(rfmod, newdata=testing)\ntest\n```\n\n## References\n\n1. Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.\nRead more: http://groupware.les.inf.puc-rio.br/har#ixzz43ag0zkAF\n\nsetwd(\"C:/Users/racevedo/Desktop/DataScienceCert/Practical Machine Learning\")\n",
    "created" : 1458697635247.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1874448477",
    "id" : "6456AC40",
    "lastKnownWriteTime" : 1458698350,
    "path" : "C:/Users/racevedo/Desktop/DataScienceCert/PML/PMLAssignment.Rmd",
    "project_path" : "PMLAssignment.Rmd",
    "properties" : {
    },
    "relative_order" : 2,
    "source_on_save" : false,
    "type" : "r_markdown"
}